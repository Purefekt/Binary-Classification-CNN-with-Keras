{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23582dc",
   "metadata": {},
   "source": [
    "# Implementing LeNet-5 with Keras\n",
    "\n",
    "I will be implementing the LeNet-5 CNN architecture with Keras in python. I will not be using TensorFlow backend since my machine has an AMD GPU, thus i will be using PlaidML backend. I will also train the model on CPU just to show how bad it is. The dataset will be CIFAR-10 which has 60k images for 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4f3e1",
   "metadata": {},
   "source": [
    "## PlaidML as backend instead of TensorFlow\n",
    "\n",
    "To use PlaidML as backend follow these steps\n",
    "- activate project environment\n",
    "- `plaidml-setup`\n",
    "- `Enable experimental device support? (y,n)[n]:n`\n",
    "- `Please choose a default device:` choose 1,2,3.. most probably 1 would be your CPU, 2 would be the integrated GPU of your CPU and 3 would be your dedicated GPU\n",
    "- `Save settings to /Users/veersingh/.plaidml? (y,n)[y]:y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d316289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries and set backend to plaidML\n",
    "from os import environ\n",
    "environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907461f",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "First we specify the paths to the directories containing our training and testing sets. Next we will rescale the images from 0-255 to 0-1 to reduce computations. Then we will use the flow_from_directory() method of the ImageDataGenerator class to resize the image to 32x32x1. This is important since LeNet-5 expects the input image to be of size 32x32 with single channel or grayscale. Our input images are of the size 32x32x3 which means we will keep the image width and height the same but change rgb to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths\n",
    "train_dataset_path = '/Users/veersingh/Desktop/cifar_10/train'\n",
    "test_dataset_path = '/Users/veersingh/Desktop/cifar_10/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598cffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Rescale\n",
    "train = ImageDataGenerator(rescale=1 / 255)\n",
    "test = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "# Define train and test dataset\n",
    "# This method is useful since our images are in their respective class folders\n",
    "train_dataset = train.flow_from_directory(seed=1, # random seed to maintain same output everytime\n",
    "                                          directory=train_dataset_path, # path to train set\n",
    "                                          target_size=(32, 32), # resize to 32x32\n",
    "                                          batch_size=32, # can be 32, 64, 128\n",
    "                                          class_mode = 'categorical', # for multi class classification\n",
    "                                          color_mode='grayscale') # change to single channel\n",
    "\n",
    "test_dataset = test.flow_from_directory(seed=1, # random seed to maintain same output everytime\n",
    "                                        directory=test_dataset_path, # path to train set\n",
    "                                        target_size=(32, 32), # resize to 32x32\n",
    "                                        batch_size=32, # can be 32, 64, 128\n",
    "                                        class_mode = 'categorical', # for multi class classification\n",
    "                                        color_mode='grayscale') # change to single channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ff928",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c12014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_5500m.0\"\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# C1: (None,32,32,1) -> (None,28,28,6).\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(32,32,1), padding='valid'))\n",
    "\n",
    "# P1: (None,28,28,6) -> (None,14,14,6).\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "# C2: (None,14,14,6) -> (None,10,10,16).\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "# P2: (None,10,10,16) -> (None,5,5,16).\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "# Flatten: (None,5,5,16) -> (None, 400).\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC1: (None, 400) -> (None,120).\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "\n",
    "# FC2: (None,120) -> (None,84).\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# FC3: (None,84) -> (None,10).\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2b8c9",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We will train the model for 30 epochs. An epoch is the number of times that the learning algorithms runs through the entire training dataset. Increasing this number will increase the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e8c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_dataset,\n",
    "                              steps_per_epoch=1560, # number of training images/batch size = 50000/32 = 1562\n",
    "                              epochs=2,\n",
    "                              validation_data=test_dataset, # validate against the test set on each epoch\n",
    "                              verbose=2) # print constant updates\n",
    "\n",
    "# optionally save the model\n",
    "# model.save('lenet_5_cifar_10.h5')\n",
    "\n",
    "# stop the timer\n",
    "time_elapsed = time.time()-start_time\n",
    "print(f'The model took {time_elapsed}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4678d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_models",
   "language": "python",
   "name": "cnn_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
