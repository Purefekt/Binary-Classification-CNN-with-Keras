{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0131c7",
   "metadata": {},
   "source": [
    "# Implementing AlexNet with Keras\n",
    "\n",
    "I will be implementing the AlexNet CNN architecture with Keras in python. I will not be using TensorFlow backend since my machine has an AMD GPU, thus i will be using PlaidML backend. I will also train the model on CPU just to show how bad it is. The dataset will be CIFAR-10 which has 60k images for 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a0723",
   "metadata": {},
   "source": [
    "## PlaidML as backend instead of TensorFlow\n",
    "\n",
    "To use PlaidML as backend follow these steps\n",
    "- activate project environment\n",
    "- `plaidml-setup`\n",
    "- `Enable experimental device support? (y,n)[n]:n`\n",
    "- `Please choose a default device:` choose 1,2,3.. most probably 1 would be your CPU, 2 would be the integrated GPU of your CPU and 3 would be your dedicated GPU\n",
    "- `Save settings to /Users/veersingh/.plaidml? (y,n)[y]:y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5997095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries and set backend to plaidML\n",
    "from os import environ\n",
    "environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b18aab",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "First we specify the paths to the directories containing our training and testing sets. Next we will rescale the images from 0-255 to 0-1 to reduce computations. Then we will use the flow_from_directory() method of the ImageDataGenerator class to resize the image to 227x227x3. This is important since AlexNet expects the input image to be of size 227x227 with 3 channels or rgb. Our input images are of the size 32x32x3 which means we will have to upscale them to 227x227 and keep the channels same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcf36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths\n",
    "train_dataset_path = '/Users/veersingh/Desktop/cifar_10/train'\n",
    "test_dataset_path = '/Users/veersingh/Desktop/cifar_10/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c20a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Rescale\n",
    "train = ImageDataGenerator(rescale=1 / 255)\n",
    "test = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "# Define train and test dataset\n",
    "# This method is useful since our images are in their respective class folders\n",
    "\n",
    "# seed = random number to maintain same output everytime\n",
    "# directory = path of train or test dataset\n",
    "# target_size = resize the input image to required shape\n",
    "# batch_size = default batch size of 32\n",
    "# class_mode = categorical for multiclass\n",
    "# color_mode = depending on number of channels required by model, grayscale for 1 channel, rgb for 3 channel\n",
    "\n",
    "train_dataset = train.flow_from_directory(seed=1,\n",
    "                                          directory=train_dataset_path,\n",
    "                                          target_size=(227, 227),\n",
    "                                          batch_size=32,\n",
    "                                          class_mode = 'categorical',\n",
    "                                          color_mode='rgb')\n",
    "\n",
    "test_dataset = test.flow_from_directory(seed=1,\n",
    "                                        directory=test_dataset_path,\n",
    "                                        target_size=(227, 227),\n",
    "                                        batch_size=32,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        color_mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6e7c",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a15acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_5500m.0\"\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(227,227,3), padding='valid'))\n",
    "\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#  output Layer, AlexNet has a 1000 neurons in the last FC layer but we have 10 classes, thus we will set it to 10\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c42158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,360,586\n",
      "Trainable params: 58,341,450\n",
      "Non-trainable params: 19,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09183c6d",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We will train the model for 30 epochs. An epoch is the number of times that the learning algorithms runs through the entire training dataset. Increasing this number will increase the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_dataset,\n",
    "                              steps_per_epoch=1560, # number of training images/batch size = 50000/32 = 1562\n",
    "                              epochs=2,\n",
    "                              validation_data=test_dataset, # validate against the test set on each epoch\n",
    "                              verbose=2) # print constant updates\n",
    "\n",
    "# optionally save the model\n",
    "# model.save('alexnet_cifar_10.h5')\n",
    "\n",
    "# stop the timer\n",
    "time_elapsed = time.time()-start_time\n",
    "print(f'The model took {time_elapsed}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187213c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_models",
   "language": "python",
   "name": "cnn_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
