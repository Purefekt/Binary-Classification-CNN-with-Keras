{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c368de3",
   "metadata": {},
   "source": [
    "# !!! CHANGE KERNEL TO -> veers_custom_cnn_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87180913",
   "metadata": {},
   "source": [
    "# Custom CNN with Keras - GPU\n",
    "\n",
    "- I built this custom CNN using Keras\n",
    "- The model was trained on AMD GPU with PlaidML backend. I did not use TensorFlow since i do not have an Nvidia GPU, but that can be changed by setting `environ[\"KERAS_BACKEND\"] = \"tensorflow\"`\n",
    "- This model can classify dogs and cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e345f",
   "metadata": {},
   "source": [
    "## Import all libraries and set backend to PlaidML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73cd82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries and set backend to plaidML\n",
    "from os import environ\n",
    "environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "import json\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3eea6",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- First we need to rescale the pixel values. Currently the pixel values are in the range [0,255] since we are using 8 bit color or 2^8 values per pixel. We can just divide each pixel value with 255 and that will rescale all pixels to the range [0,1]. This reduces computation.\n",
    "- Next we need to label and resize the image since the model accepts images of the fixed size 100x100x3 where the image is 100 pixels in length and width and has 3 channels RGB. We can do this automatically by using ImageDataGenerator which has the method flow_from_director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e738b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths\n",
    "train_dataset_path = '/Users/veersingh/Desktop/cats_dogs/train'\n",
    "valid_dataset_path = '/Users/veersingh/Desktop/cats_dogs/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4191212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 2 classes.\n",
      "Found 7000 images belonging to 2 classes.\n",
      "\n",
      "\n",
      "Label for cat -> 0\n",
      "Label for dog -> 1\n"
     ]
    }
   ],
   "source": [
    "# Rescale\n",
    "train = ImageDataGenerator(rescale=1 / 255)\n",
    "valid = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "# Define train and valid dataset\n",
    "# This method is useful since our images are in their respective class folders\n",
    "\n",
    "# seed = random number to maintain same output everytime\n",
    "# directory = path of train or valid dataset\n",
    "# target_size = resize the input image to required shape\n",
    "# batch_size = default batch size of 32\n",
    "# class_mode = categorical for multiclass\n",
    "# color_mode = depending on number of channels required by model, grayscale for 1 channel, rgb for 3 channel\n",
    "\n",
    "train_dataset = train.flow_from_directory(seed=1,\n",
    "                                          directory=train_dataset_path,\n",
    "                                          target_size=(100, 100),\n",
    "                                          batch_size=32,\n",
    "                                          class_mode = 'binary',\n",
    "                                          color_mode='rgb')\n",
    "\n",
    "valid_dataset = valid.flow_from_directory(seed=1,\n",
    "                                         directory=valid_dataset_path,\n",
    "                                         target_size=(100, 100),\n",
    "                                         batch_size=32,\n",
    "                                         class_mode = 'binary',\n",
    "                                         color_mode='rgb')\n",
    "\n",
    "# print labels\n",
    "print('\\n')\n",
    "print(f'Label for cat -> {train_dataset.class_indices[\"cat\"]}')\n",
    "print(f'Label for dog -> {train_dataset.class_indices[\"dog\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1f8bd",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bd2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_5500m.0\"\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3),activation='relu',input_shape=(100, 100, 3)))\n",
    "\n",
    "# Maxpool layer 1\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Maxpool layer 2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Maxpool layer 3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional layer 4\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Maxpool layer 4\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten to 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer with 512 neurons\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "# Output layer with single neuron which gives 0 for cat and 1 for dog\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73ad0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03eba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 47, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,290,433\n",
      "Trainable params: 1,290,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf3f06",
   "metadata": {},
   "source": [
    "## Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576aafbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veersingh/opt/anaconda3/envs/veers_custom_cnn_env/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 96s - loss: 0.6617 - acc: 0.5931 - val_loss: 0.6110 - val_acc: 0.6623\n",
      "Epoch 2/30\n",
      " - 95s - loss: 0.5863 - acc: 0.6858 - val_loss: 0.5388 - val_acc: 0.7200\n",
      "Epoch 3/30\n",
      " - 95s - loss: 0.4979 - acc: 0.7563 - val_loss: 0.4701 - val_acc: 0.7746\n",
      "Epoch 4/30\n",
      " - 95s - loss: 0.4283 - acc: 0.8014 - val_loss: 0.5033 - val_acc: 0.7689\n",
      "Epoch 5/30\n",
      " - 95s - loss: 0.3712 - acc: 0.8362 - val_loss: 0.3744 - val_acc: 0.8281\n",
      "Epoch 6/30\n",
      " - 95s - loss: 0.3312 - acc: 0.8548 - val_loss: 0.3731 - val_acc: 0.8359\n",
      "Epoch 7/30\n",
      " - 95s - loss: 0.2937 - acc: 0.8727 - val_loss: 0.3533 - val_acc: 0.8449\n",
      "Epoch 8/30\n",
      " - 95s - loss: 0.2413 - acc: 0.8996 - val_loss: 0.3672 - val_acc: 0.8500\n",
      "Epoch 9/30\n",
      " - 94s - loss: 0.1912 - acc: 0.9187 - val_loss: 0.3551 - val_acc: 0.8551\n",
      "Epoch 10/30\n",
      " - 93s - loss: 0.1528 - acc: 0.9382 - val_loss: 0.4024 - val_acc: 0.8566\n",
      "Epoch 11/30\n",
      " - 93s - loss: 0.1146 - acc: 0.9548 - val_loss: 0.3717 - val_acc: 0.8571\n",
      "Epoch 12/30\n",
      " - 96s - loss: 0.0881 - acc: 0.9667 - val_loss: 0.4581 - val_acc: 0.8571\n",
      "Epoch 13/30\n",
      " - 95s - loss: 0.0914 - acc: 0.9656 - val_loss: 0.4928 - val_acc: 0.8536\n",
      "Epoch 14/30\n",
      " - 96s - loss: 0.0795 - acc: 0.9692 - val_loss: 0.5722 - val_acc: 0.8476\n",
      "Epoch 15/30\n",
      " - 96s - loss: 0.0521 - acc: 0.9819 - val_loss: 0.5293 - val_acc: 0.8566\n",
      "Epoch 16/30\n",
      " - 96s - loss: 0.0362 - acc: 0.9871 - val_loss: 0.6719 - val_acc: 0.8363\n",
      "Epoch 17/30\n",
      " - 96s - loss: 0.0591 - acc: 0.9784 - val_loss: 0.5620 - val_acc: 0.8527\n",
      "Epoch 18/30\n",
      " - 95s - loss: 0.0262 - acc: 0.9907 - val_loss: 0.6324 - val_acc: 0.8596\n",
      "Epoch 19/30\n",
      " - 92s - loss: 0.0373 - acc: 0.9867 - val_loss: 0.7502 - val_acc: 0.8450\n",
      "Epoch 20/30\n",
      " - 92s - loss: 0.0322 - acc: 0.9882 - val_loss: 0.7145 - val_acc: 0.8604\n",
      "Epoch 21/30\n",
      " - 91s - loss: 0.0392 - acc: 0.9862 - val_loss: 0.6448 - val_acc: 0.8441\n",
      "Epoch 22/30\n",
      " - 91s - loss: 0.0365 - acc: 0.9874 - val_loss: 0.7098 - val_acc: 0.8611\n",
      "Epoch 23/30\n",
      " - 91s - loss: 0.0241 - acc: 0.9924 - val_loss: 0.6864 - val_acc: 0.8559\n",
      "Epoch 24/30\n",
      " - 96s - loss: 0.0412 - acc: 0.9858 - val_loss: 0.5640 - val_acc: 0.8514\n",
      "Epoch 25/30\n",
      " - 96s - loss: 0.0232 - acc: 0.9923 - val_loss: 0.8568 - val_acc: 0.8471\n",
      "Epoch 26/30\n",
      " - 96s - loss: 0.0368 - acc: 0.9872 - val_loss: 0.7384 - val_acc: 0.8551\n",
      "Epoch 27/30\n",
      " - 96s - loss: 0.0278 - acc: 0.9904 - val_loss: 0.7426 - val_acc: 0.8540\n",
      "Epoch 28/30\n",
      " - 96s - loss: 0.0341 - acc: 0.9885 - val_loss: 0.8560 - val_acc: 0.8446\n",
      "Epoch 29/30\n",
      " - 96s - loss: 0.0256 - acc: 0.9918 - val_loss: 0.7752 - val_acc: 0.8619\n",
      "Epoch 30/30\n",
      " - 95s - loss: 0.0190 - acc: 0.9927 - val_loss: 0.9040 - val_acc: 0.8503\n",
      "\n",
      "\n",
      "The model took 2839.06 seconds to train\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# generator = input training data created using ImageDataGenerator\n",
    "# steps_per_epoch = number of total training images/batch size = 16000/32 = 500\n",
    "# epochs = number of times we go through the entire training dataset\n",
    "# validation_data = validation dataset\n",
    "# verbose = it will print updates on every epoch\n",
    "\n",
    "history = model.fit_generator(generator=train_dataset,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=30,\n",
    "                              validation_data=valid_dataset,\n",
    "                              verbose=2)\n",
    "\n",
    "# save the model\n",
    "model.save('models/trained_model_gpu.h5')\n",
    "\n",
    "# stop the timer\n",
    "time_elapsed = round(time.time()-start_time, 2)\n",
    "\n",
    "print(f'\\n\\nThe model took {time_elapsed} seconds to train' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5d74e",
   "metadata": {},
   "source": [
    "## Create log to plot graphs and serialize to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088153c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = history.history\n",
    "\n",
    "# add time to train to log\n",
    "log['time_to_train_gpu'] = time_elapsed\n",
    "\n",
    "\n",
    "with open(\"log_gpu.json\", \"w\") as data_file:\n",
    "    json.dump(log, data_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veers_custom_cnn_env",
   "language": "python",
   "name": "veers_custom_cnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
